import os
import json
from os.path import join, basename
import configparser

from SCons.Script import Command
from bioscons.fileutils import rename

##### inputs #######
# TODO: move to a config file

bei_data = '/mnt/disk15/molmicro/working/ngh2/2018-08-08-bei-refset'
refpkg = join(bei_data, 'mkrefpkg/output/bei-hm27-plus/bei-hm27-plus-1.0.refpkg')

# data_conf = 'data/data-small.conf'
data_conf = 'data/data.conf'
fastq_data = configparser.ConfigParser(allow_no_value=True)
fastq_data.read(data_conf)
sections = list(fastq_data.items())
sections.pop(0)  # first section is DEFAULT
fastq_files = [(name, s['r1'], s['r2']) for name, s in sections]

# known classifications
seq_info = '$input/data/seq_info.csv'

capture_data = '/molmicro/working/sara'
binds = ','.join([bei_data, capture_data])

##### end inputs ###

Decider('MD5-timestamp')
vars = Variables()
vars.Add('out', '', 'output-capture')

# Define some PATH elements explicitly.
venv = os.environ['VIRTUAL_ENV']
PATH=':'.join([
    'bin',
    join(venv, 'bin'),
    '/app/bin',  # provides R
    '/usr/local/bin', '/usr/bin', '/bin'])

env = Environment(
    ENV=dict(os.environ, PATH=PATH),
    variables=vars,
    SHELL='bash',
    cwd=os.getcwd(),
    binds=binds,
    epa=('singularity run --pwd $cwd -B $cwd,$binds epa.simg'),
    gappa=('singularity run --pwd $cwd -B $cwd,$binds gappa.simg'),
    krona=('singularity run --pwd $cwd -B $cwd,$binds krona.simg'),
    taxit=('singularity exec '
           '--pwd $cwd -B $cwd,$binds '
           '/molmicro/common/singularity/taxtastic-0.8.5-singularity2.4.img '
           'taxit'),
    fastq_mcf=('singularity exec '
               '--pwd $cwd -B $cwd,$binds ea-utils.simg fastq-mcf'),
    super_deduper=('singularity exec '
                   '--pwd $cwd -B $cwd htstream.simg hts_SuperDeduper'),
    deenurp_img=('singularity exec '
                 '--pwd $cwd -B $cwd,$binds '
                 '/molmicro/common/singularity/deenurp-v0.2.4-singularity2.4.img'),
    nproc=15,
)

def get_refpkg_contents(refpkg):
    with open(join(refpkg, 'CONTENTS.json')) as jfile:
        files = json.load(jfile)['files']
        return {k: join(refpkg, v) for k, v in files.items()}


refpkg_files = get_refpkg_contents(refpkg)
ref_msa = refpkg_files['aln_fasta']
ref_sto = refpkg_files['aln_sto']
tree = refpkg_files['tree']
tree_stats = refpkg_files['tree_stats']
ref_info = refpkg_files['seq_info']
ref_taxonomy = refpkg_files['taxonomy']
profile = refpkg_files['profile']


## begin analysis

taxon_file = env.Command(
    target='$out/taxonomy.txt',
    source=[ref_taxonomy, ref_info],
    action='$taxit lineage_table $SOURCES --taxonomy-table $TARGET'
)

krona_data = []
for_counts = []
for label, r1, r2 in fastq_files:
    e = env.Clone(
        label=label,
        out='$out/$label',
        max_reads=100000,
    )
    for_counts.append(r1)

    # downsample
    r1 = e.Command(
        target='$out/r1_downsampled.fastq',
        source=r1,
        action='seqmagick convert --head ${max_reads} $SOURCE $TARGET'
    )
    for_counts.append(r1)

    r2 = e.Command(
        target='$out/r2_downsampled.fastq',
        source=r2,
        action='seqmagick convert --head ${max_reads} $SOURCE $TARGET'
    )

    # trim and quality filter
    r1_cleaned, r2_cleaned, fqmcf_log = e.Command(
        target=['$out/cleaned_R1.fastq.gz', '$out/cleaned_R2.fastq.gz', '$out/fastq_mcf.log'],
        source=['data/Illumina_adaptors_v2.fa', r1, r2],
        action=('$fastq_mcf -o ${TARGETS[0]} -o ${TARGETS[1]} '
                '-D 0 -k 0 -q 5 -l 25 $SOURCES '
                '1> ${TARGETS[2]}')
    )
    for_counts.append(r1_cleaned)

    # remove PCR duplicates
    r1_deduped, r2_deduped, dedup_log = e.Command(
        target=['$out/deduped_R1.fastq',
                '$out/deduped_R2.fastq',
                '$out/super_deduper.log'],
        source=[r1_cleaned, r2_cleaned],
        action=('$super_deduper --force '
                '--prefix $out/deduped '
                '--read1-input ${SOURCES[0]} '
                '--read2-input ${SOURCES[1]} '
                '--stats-file ${TARGETS[2]} ')
    )
    for_counts.append(r1_deduped)

    # assemble paired reads
    assembled_fq, discarded, r1_unassembled_fq, r2_unassembled_fq = e.Command(
        target=['$out/pear.assembled.fastq',
                '$out/pear.discarded.fastq',
                '$out/pear.unassembled.forward.fastq',
                '$out/pear.unassembled.reverse.fastq'],
        source=[r1_deduped, r2_deduped],
        action=('pear -f ${SOURCES[0]} -r ${SOURCES[1]} '
                '--min-trim-length 25 '
                '--threads 4 '
                '--output $out/pear ')
        )
    for_counts.extend([assembled_fq, discarded, r1_unassembled_fq])

    # convert to fasta
    assembled = e.Command(
        target=rename(assembled_fq, ext='.fasta'),
        source=assembled_fq,
        action='seqmagick convert $SOURCE $TARGET')

    r1_unassembled = e.Command(
        target=rename(r1_unassembled_fq, ext='.fasta'),
        source=r1_unassembled_fq,
        action='seqmagick convert --name-suffix _r1 $SOURCE $TARGET')

    r2_unassembled = e.Command(
        target=rename(r2_unassembled_fq, ext='.fasta'),
        source=r2_unassembled_fq,
        action='seqmagick convert --name-suffix _r2 $SOURCE $TARGET')

    # concatenate into a single file
    unfiltered = e.Command(
        target='$out/unfiltered.fasta',
        source=[
            assembled,
            r1_unassembled, r2_unassembled],
        action='cat $SOURCES > $TARGET'
    )

    # remove non-16s reads
    seqs_16s, seqs_not16s, cmsearch_scores = e.Command(
        target=['$out/seqs-16s.fasta',
                '$out/seqs-not16s.fasta',
                '$out/cmsearch_scores.txt'],
        source=[unfiltered, 'data/RRNA_16S_BACTERIA.calibrated.cm'],
        action=('$deenurp_img '
                'bin/cmfilter.py $SOURCES '
                '--outfile ${TARGETS[0]} '
                '--discarded ${TARGETS[1]} '
                '--scores ${TARGETS[2]} '
                '--min-evalue 0.01 '
                '--cpu $nproc '
                '--reverse-complement ')
    )
    Depends(seqs_16s, 'bin/cmfilter.py')
    for_counts.extend([seqs_16s, seqs_not16s])

    # align input seqs with cmalign
    query_sto, cmalign_scores = e.Command(
        target=['$out/query.sto', '$out/cmalign.scores'],
        source=[seqs_16s, profile],
        # ncores=args.nproc,
        # timelimit=30,
        # slurm_args = '--mem=130000',
        # slurm_queue=large_queue,
        action=(
            '$deenurp_img '
            'cmalign '
            '--cpu $nproc '
            '--mxsize 8196 '
            '--noprob '
            '--dnaout '
            '-o ${TARGETS[0]} '  # alignment in stockholm format
            '--sfile ${TARGETS[1]} '  # scores
            '${SOURCES[1]} '  # alignment profile
            '${SOURCES[0]} '  # input fasta file
            '| grep -E "^#"'  # limit stdout to commented lines
        ))

    # place ref and query alignments into the same alignemnt register
    refalign, qalign_unmerged = e.Command(
        target=['$out/refalign.fasta', '$out/qalign_unmerged.fasta'],
        source=[ref_sto, query_sto],
        action=('$deenurp_img bin/merge.py $SOURCES $TARGETS')
    )
    Depends(refalign, 'bin/merge.py')

    # calculate a consensus for unassembled read pairs
    qalign = e.Command(
        target='$out/qalign.fasta',
        source=qalign_unmerged,
        action='bin/get_consensus.py $SOURCE -o $TARGET'
    )
    Depends(qalign, 'bin/get_consensus.py')

    # place reads onto the reference tree
    epa_placements, epa_log = e.Command(
        target=['$out/epa_result.jplace', '$out/epa_info.log'],
        source=[refalign, tree,
                # qalign_unmerged,
                qalign,
                tree_stats],
        action=('$epa '
                '--ref-msa ${SOURCES[0]} '
                '--tree ${SOURCES[1]} '
                '--query ${SOURCES[2]} '
                '--model `python bin/get_model_descriptor.py ${SOURCES[3]}` '
                '--outdir $out')
    )

    # classify
    # names of gappa targets appear to be hard-coded
    labelled_tree, per_pquery_assign, gappa_profile, krona_raw = e.Command(
        target=['$out/labelled_tree', '$out/per_pquery_assign',
                '$out/profile.csv', '$out/krona-${label}-raw'],
        source=[epa_placements, taxon_file],
        action=[('$gappa analyze assign '
                '--krona '
                '--out-dir $out '
                '--jplace-path ${SOURCES[0]} '
                 '--taxon-file ${SOURCES[1]}'),
                'mv $out/krona.profile ${TARGETS[3]}']
    )
    krona_data.append(krona_raw)

    # filter by likelihood and reformat
    classifications, lineages, krona_filtered = e.Command(
        target=['$out/classifications.csv', '$out/lineages.csv', '$out/krona-${label}-filtered'],
        source=per_pquery_assign,
        action=('bin/get_classifications.py $SOURCE '
                '--classifications ${TARGETS[0]} '
                '--lineages ${TARGETS[1]} '
                '--krona ${TARGETS[2]} '
                '--min-afract 0.5 '
                '--min-total 0.75 '
        )
    )
    Depends(classifications, 'bin/get_classifications.py')
    krona_data.append(krona_filtered)

    krona = e.Command(
        target='$out/krona.html',
        source=[krona_raw, krona_filtered],
        action='$krona -o $TARGET $SOURCES'
    )

# count reads in all fastq files
read_counts = env.Command(
    target='$out/read_counts.csv',
    source=Flatten(for_counts),
    action='bin/read_depth.py -j $nproc $SOURCES -o $TARGET'
)

read_stats = env.Command(
    target='$out/read_stats.csv',
    source=read_counts,
    action='bin/read_stats.py $SOURCE -o $TARGET'
)

if krona_data:
    # all krona plots
    krona_all = env.Command(
        target='$out/krona.html',
        source=krona_data,
        action='$krona -o $TARGET $SOURCES'
    )
